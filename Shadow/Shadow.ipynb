{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# _*_ coding:utf-8 _*_\n",
    "r\"\"\"----------------------------------------------------------------------------\n",
    "@File    : ASDESH_Spl1001_Feat9_Algo2.py\n",
    "@Time    : 2022/11/29 09:44:52\n",
    "@Author  : Zheng Han\n",
    "@Contact : hzsongrentou1580@gmail.com\n",
    "@License : (C)Copyright 2022, ZhengHan. All rights reserved.\n",
    "@Desc    : RemoteShadow of Test1_classify\n",
    "-----------------------------------------------------------------------------\"\"\"\n",
    "import json\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "pd.set_option('display.width', 500)  # 数据显示总宽度\n",
    "pd.set_option('display.max_columns', 10)  # 显示最多列数，超出该数以省略号表示\n",
    "pd.set_option('display.max_colwidth', 16)  # 设置单列的宽度，用字符个数表示，单个数据长度超出该数时以省略号表示\n",
    "np.set_printoptions(linewidth=500, precision=8, suppress=True)\n",
    "\n",
    "with open(r\"ASDESH_Spl1001_Feat9_Algo2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    save_d = json.load(f)\n",
    "# 数据中所有特征的名字\n",
    "feat_names_all = np.array(save_d[\"feat_names_all\"])\n",
    "# 数据伸缩的参数\n",
    "re_hist = np.array(save_d[\"re_hist\"])\n",
    "# 特征类型的名称\n",
    "feat_type_names = np.array(save_d[\"feat_type_names\"])\n",
    "# ENVI Class 文件信息\n",
    "envi_class_file_info = save_d[\"envi_class_file_info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![sadfhsdjk](C:\\Users\\Administrator.DESKTOP-GH3VHED\\Desktop\\QQ截图20221113112840.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# SRTUtils\n",
    "\n",
    "class EnviFileIO:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, im_file=None, im_info_json_file=\"\"):\n",
    "        self.im_file = im_file\n",
    "        self.im_infos = None\n",
    "        if im_info_json_file == \"\" and im_file is not None:\n",
    "            self.setInfoFromHDRFile(os.path.splitext(im_file)[0] + \".hdr\")\n",
    "        else:\n",
    "            if im_info_json_file != \"\":\n",
    "                self.setInfoFromJsonFile(im_info_json_file)\n",
    "        self.d = None\n",
    "        pass\n",
    "\n",
    "    def setInfoFromJsonFile(self, json_file):\n",
    "        \"\"\" 使用json文件获得保存影像的信息\n",
    "\n",
    "        :param json_file: json文件\n",
    "        :return: 信息\n",
    "        \"\"\"\n",
    "        self.im_infos = None\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as fr:\n",
    "            self.im_infos = json.load(fr)\n",
    "        return self.im_infos\n",
    "\n",
    "    def setInfoFromHDRFile(self, hdr_file):\n",
    "        \"\"\" 使用hdr文件获得保存影像的信息\n",
    "\n",
    "        :param hdr_file: hdr文件\n",
    "        :return: 信息\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        with open(hdr_file, \"r\", encoding=\"utf-8\") as fr:\n",
    "            for line in fr:\n",
    "                if line.strip() == \"ENVI\":\n",
    "                    continue\n",
    "                if line.find(\"=\") != -1:\n",
    "                    lines.append(line)\n",
    "                else:\n",
    "                    lines[-1] += line\n",
    "        self.im_infos = {}\n",
    "        for line in lines:\n",
    "            line1 = line.split(\"=\", 2)\n",
    "            self.im_infos[line1[0].strip()] = line1[1].strip()\n",
    "        return self.im_infos\n",
    "\n",
    "    def readToArray(self, interleave=\"r,c,b\"):\n",
    "        \"\"\"\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n_rows = int(self.im_infos[\"lines\"])\n",
    "        n_columns = int(self.im_infos[\"samples\"])\n",
    "        n_bands = int(self.im_infos[\"bands\"])\n",
    "        n_data = n_rows * n_columns * n_bands\n",
    "        with open(self.im_file, \"rb\") as frb:\n",
    "            if self.im_infos[\"data type\"] == \"1\":\n",
    "                self.d = struct.unpack(str(n_data) + \"B\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"2\":\n",
    "                self.d = struct.unpack(str(n_data) + \"h\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"3\":\n",
    "                self.d = struct.unpack(str(n_data) + \"i\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"4\":\n",
    "                self.d = struct.unpack(str(n_data) + \"f\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"5\":\n",
    "                self.d = struct.unpack(str(n_data) + \"d\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"6\":\n",
    "                self.d = struct.unpack(str(n_data * 2) + \"f\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"9\":\n",
    "                self.d = struct.unpack(str(n_data * 2) + \"d\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"12\":\n",
    "                self.d = struct.unpack(str(n_data) + \"H\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"13\":\n",
    "                self.d = struct.unpack(str(n_data * 2) + \"I\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"14\":\n",
    "                self.d = struct.unpack(str(n_data * 2) + \"q\", frb.read())\n",
    "            elif self.im_infos[\"data type\"] == \"15\":\n",
    "                self.d = struct.unpack(str(n_data * 2) + \"Q\", frb.read())\n",
    "            else:\n",
    "                raise Exception(\"Can not find \\\"data type\\\"=\" + self.im_infos[\"data type\"])\n",
    "        self.d = np.array(self.d)\n",
    "        if self.im_infos[\"interleave\"].lower() == \"bsq\":\n",
    "            self.d = self.d.reshape([n_bands, n_rows, n_columns])\n",
    "            if interleave == \"r,c,b\":\n",
    "                self.d = np.transpose(self.d, axes=(1, 2, 0))\n",
    "        elif self.im_infos[\"interleave\"].lower() == \"bip\":\n",
    "            self.d = self.d.reshape([n_rows, n_columns, n_bands])\n",
    "            if interleave == \"b,r,c\":\n",
    "                self.d = np.transpose(self.d, axes=(2, 0, 1))\n",
    "        elif self.im_infos[\"interleave\"].lower() == \"bil\":\n",
    "            self.d = self.d.reshape([n_rows, n_bands, n_columns])\n",
    "            if interleave == \"b,r,c\":\n",
    "                self.d = np.transpose(self.d, axes=(1, 0, 2))\n",
    "            if interleave == \"r,c,b\":\n",
    "                self.d = np.transpose(self.d, axes=(0, 2, 1))\n",
    "        return self.d\n",
    "\n",
    "    def infoToJson(self, json_file):\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "            json.dump(self.im_infos, fw)\n",
    "        return self.im_infos\n",
    "\n",
    "    def saveToFile(self, imd, im_infos=None, out_file=None):\n",
    "        if out_file is None:\n",
    "            out_file = self.im_file\n",
    "        if im_infos is None:\n",
    "            im_infos = self.im_infos\n",
    "        to_f = os.path.splitext(out_file)[0]\n",
    "        with open(to_f + \".hdr\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"ENVI\\n\")\n",
    "            for k in im_infos:\n",
    "                f.write(k + \" = \" + im_infos[k] + \"\\n\")\n",
    "        with open(out_file, \"wb\") as f:\n",
    "            imd.tofile(f)\n",
    "        return im_infos\n",
    "\n",
    "    @classmethod\n",
    "    def hdr2ImInfo(cls, hdr_file, im_info_json_file):\n",
    "        \"\"\" ENVI头文件转信息文件\n",
    "\n",
    "        :param hdr_file: ENVI头文件\n",
    "        :param im_info_json_file:信息文件\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        with open(hdr_file, \"r\", encoding=\"utf-8\") as fr:\n",
    "            for line in fr:\n",
    "                if line.strip() == \"ENVI\":\n",
    "                    continue\n",
    "                if line.find(\"=\") != -1:\n",
    "                    lines.append(line)\n",
    "                else:\n",
    "                    lines[-1] += line\n",
    "        infos = {}\n",
    "        for line in lines:\n",
    "            line1 = line.split(\"=\", 2)\n",
    "            infos[line1[0].strip()] = line1[1].strip()\n",
    "        with open(im_info_json_file, \"w\", encoding=\"utf-8\") as fw:\n",
    "            json.dump(infos, fw)\n",
    "        return infos\n",
    "\n",
    "\n",
    "def plotImageHist(imd: np.array, bands=None, labels=None):\n",
    "    \"\"\" 绘制图像的波段数据分布图\n",
    "\n",
    "    :param imd: 图像数据\n",
    "    :param bands: 绘制的波段\n",
    "    :param labels: 显示的标签\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if bands is None:\n",
    "        bands = [i for i in range(imd.shape[0])]\n",
    "    if labels is None:\n",
    "        labels = [\"Band \" + str(i + 1) for i in range(imd.shape[0])]\n",
    "\n",
    "    for i, i_band in enumerate(bands):\n",
    "        h, bin_edges = np.histogram(imd[i_band], bins=256)\n",
    "        plt.plot(h, label=labels[i])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def saveCM(cm, save_cm_file, cate_names=None, infos=None):\n",
    "    \"\"\" 保存混淆矩阵\n",
    "\n",
    "    :param cm: 混淆矩阵\n",
    "    :param save_cm_file: 混淆矩阵文件\n",
    "    :param cate_names: 混淆矩阵类别名\n",
    "    :param infos: 标识信息\n",
    "    :return: 写到第几个了\n",
    "    \"\"\"\n",
    "    if isinstance(infos, list):\n",
    "        infos = \" \".join(infos)\n",
    "    if infos is None:\n",
    "        infos = \"\"\n",
    "    n_split = len(str(np.max(cm)))\n",
    "    if n_split < 5:\n",
    "        n_split = 5\n",
    "    n_split += 1\n",
    "    if cate_names is None:\n",
    "        cate_names = [\"C \" + str(i) for i in range(cm.shpe[0])]\n",
    "    for c in cate_names:\n",
    "        if len(c) > n_split:\n",
    "            n_split = len(c)\n",
    "    if os.path.isfile(save_cm_file):\n",
    "        with open(save_cm_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            n = 0\n",
    "            for line in f:\n",
    "                if len(line) >= 2:\n",
    "                    if line[:2] == \"> \":\n",
    "                        n += 1\n",
    "            n += 1\n",
    "    else:\n",
    "        n = 1\n",
    "    out_str = fmtCM(cm, cate_names)\n",
    "    with open(save_cm_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"> \" + str(n) + \" \" + infos + \"\\n\")\n",
    "        f.write(out_str)\n",
    "    return n\n",
    "\n",
    "\n",
    "def fmtCM(cm: np.array, cate_names):\n",
    "    fmt_row0 = \"{:>8}\"\n",
    "    fmt_column0 = \"{:>8}\"\n",
    "    fmt_number = \"{:>8d}\"\n",
    "    fmt_float = \"{:>8.2f}\"\n",
    "    n_cate = len(cate_names)\n",
    "    out_s = \"\"\n",
    "    out_s += fmt_column0.format(\"CM\")\n",
    "    for i in range(n_cate):\n",
    "        out_s += \" \" + fmt_row0.format(cate_names[i])\n",
    "    out_s += \" \" + fmt_row0.format(\"SUM\")\n",
    "    out_s += \" \" + fmt_row0.format(\"PA\") + \"\\n\"\n",
    "    for i in range(n_cate):\n",
    "        out_s += fmt_column0.format(cate_names[i])\n",
    "        for j in range(n_cate):\n",
    "            out_s += \" \" + fmt_number.format(int(cm[i, j]))\n",
    "        out_s += \" \" + fmt_number.format(int(cm[i, n_cate]))\n",
    "        out_s += \" \" + fmt_float.format(cm[i, n_cate + 1]) + \"\\n\"\n",
    "    out_s += fmt_column0.format(\"SUM\")\n",
    "    for i in range(n_cate):\n",
    "        out_s += \" \" + fmt_number.format(int(cm[n_cate, i]))\n",
    "    out_s += \" \" + fmt_number.format(int(cm[n_cate, n_cate]))\n",
    "    out_s += \" \" + fmt_float.format(cm[n_cate, n_cate + 1]) + \"\\n\"\n",
    "    out_s += fmt_column0.format(\"UA\")\n",
    "    for i in range(n_cate):\n",
    "        out_s += \" \" + fmt_float.format(cm[n_cate + 1, i])\n",
    "    out_s += \" \" + fmt_float.format(cm[n_cate + 1, n_cate])\n",
    "    out_s += \" \" + fmt_float.format(cm[n_cate + 1, n_cate + 1]) + \"\\n\"\n",
    "    return out_s\n",
    "\n",
    "\n",
    "def printTable(d: np.array, columns_names=None, row_index=None, precision=2, alignment=\"right\"):\n",
    "    n_rows, n_colums = d.shape\n",
    "    column_info = []\n",
    "    # 获得行宽\n",
    "    if columns_names is not None:\n",
    "        for i in range(n_colums):\n",
    "            column_info.append([len(columns_names[i]), precision])\n",
    "    else:\n",
    "        for i in range(n_colums):\n",
    "            column_info.append([0, precision])\n",
    "    d_max = np.max(d, axis=0)\n",
    "    for i in range(n_colums):\n",
    "        w = len(str(int(d_max[i]))) + precision\n",
    "        if w > column_info[i][0]:\n",
    "            column_info[i][0] = w + 2\n",
    "    fmts = []\n",
    "    for i in range(n_colums):\n",
    "        fmt = \":\"\n",
    "        if alignment == \"center\":\n",
    "            fmt += \"^\"\n",
    "        elif alignment == \"left\":\n",
    "            fmt += \"<\"\n",
    "        else:\n",
    "            fmt += \">\"\n",
    "        fmt += str(column_info[i][0]) + \".\" + str(precision) + \"f\"\n",
    "        fmts.append(\"{\" + fmt + \"}\")\n",
    "    w = len(str(int(n_rows))) + 1\n",
    "    fmt_index = \"{:>\" + str(w) + \"d}\"\n",
    "    line0 = \"|\" + \" \" * w + \" | \"\n",
    "    for j in range(n_colums):\n",
    "        fmt = \"{:\"\n",
    "        if alignment == \"center\":\n",
    "            fmt += \"^\"\n",
    "        elif alignment == \"left\":\n",
    "            fmt += \"<\"\n",
    "        else:\n",
    "            fmt += \">\"\n",
    "        fmt += str(column_info[j][0])\n",
    "        fmt += \"}\"\n",
    "        if columns_names is not None:\n",
    "            line0 += fmt.format(columns_names[j]) + \" | \"\n",
    "        else:\n",
    "            line0 += fmt.format(\" \") + \" | \"\n",
    "\n",
    "    line0 = line0[:-1]\n",
    "    line1 = \"\"\n",
    "    for c in line0:\n",
    "        if c == \"|\":\n",
    "            line1 += \"+\"\n",
    "        else:\n",
    "            line1 += \"-\"\n",
    "    if columns_names is None:\n",
    "        print(line1)\n",
    "    else:\n",
    "        print(line1)\n",
    "        print(line0)\n",
    "        print(line1)\n",
    "    for i in range(n_rows):\n",
    "        print(\"\", end=\"|\")\n",
    "        print(fmt_index.format(i + 1), end=\" | \")\n",
    "        for j in range(n_colums):\n",
    "            print(fmts[j].format(d[i, j]), end=\" | \")\n",
    "        print()\n",
    "    print(line1)\n",
    "    pass\n",
    "\n",
    "\n",
    "def reHist(d, ratio=0.001):\n",
    "    n_re = d.shape[1] * d.shape[2] * ratio\n",
    "    d0, d1 = [], []\n",
    "    d00, d10 = 0, 0\n",
    "    for i in range(d.shape[0]):\n",
    "        d_i = d[i]\n",
    "        print(i, \":\", \"-\" * 80)\n",
    "        while True:\n",
    "            k1, k2 = 0, 0\n",
    "            zuo, you = 0, 0\n",
    "            h, bin_edges = np.histogram(d_i, bins=256)\n",
    "            for j in range(h.shape[0]):\n",
    "                zuo += h[j]\n",
    "                k1 += 1\n",
    "                if k1 == 10:\n",
    "                    break\n",
    "                if zuo >= n_re:\n",
    "                    d00 = bin_edges[j]\n",
    "                    break\n",
    "            for j in range(h.shape[0] - 1, -1, -1):\n",
    "                you += h[j]\n",
    "                k2 += 1\n",
    "                if k2 == 10:\n",
    "                    break\n",
    "                if you >= n_re:\n",
    "                    d10 = bin_edges[j + 1]\n",
    "                    break\n",
    "            print(k1, d00, k2, d10)\n",
    "            if k1 != 10 and k2 != 10:\n",
    "                d0.append(d00)\n",
    "                d1.append(d10)\n",
    "                break\n",
    "            d_i = np.clip(d_i, d00, d10)\n",
    "    return np.array([d0, d1]).T\n",
    "\n",
    "\n",
    "def calCM(in_cm: np.array):\n",
    "    \"\"\" 混淆矩阵\n",
    "\n",
    "    :param in_cm: 输入混淆矩阵\n",
    "    :return: 混淆矩阵\n",
    "    \"\"\"\n",
    "    n_class = in_cm.shape[0]\n",
    "    out_cm = np.zeros([n_class + 2, n_class + 2])\n",
    "    out_cm[:n_class, :n_class] = in_cm\n",
    "    out_cm[n_class, :] = np.sum(out_cm, axis=0)\n",
    "    out_cm[:, n_class] = np.sum(out_cm, axis=1)\n",
    "    out_cm[n_class + 1, :] = np.diag(out_cm) * 1.0 / (out_cm[n_class, :] + 0.0000001) * 100\n",
    "    out_cm[:, n_class + 1] = np.diag(out_cm) * 1.0 / (out_cm[:, n_class] + 0.0000001) * 100\n",
    "    out_cm[n_class + 1, n_class + 1] = (np.sum(np.diag(in_cm))) / out_cm[n_class, n_class] * 100\n",
    "    return out_cm\n",
    "\n",
    "\n",
    "class Jdt:\n",
    "    \"\"\"\n",
    "    进度条\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, total=100, desc=None, iterable=None, n_cols=20):\n",
    "        \"\"\" 初始化一个进度条对象\n",
    "\n",
    "        :param iterable: 可迭代的对象, 在手动更新时不需要进行设置\n",
    "        :param desc: 字符串, 左边进度条描述文字\n",
    "        :param total: 总的项目数\n",
    "        :param n_cols: 调整进度条宽度, 默认是根据环境自动调节长度, 如果设置为0, 就没有进度条, 只有输出的信息\n",
    "        \"\"\"\n",
    "        self.total = total\n",
    "        self.iterable = iterable\n",
    "        self.n_cols = n_cols\n",
    "        self.desc = desc if desc is not None else \"\"\n",
    "\n",
    "        self.n_split = float(total) / float(n_cols)\n",
    "        self.n_current = 0\n",
    "        self.n_print = 0\n",
    "        self.is_run = False\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\" 开始进度条 \"\"\"\n",
    "        self.is_run = True\n",
    "        print()\n",
    "\n",
    "    def add(self, n=1):\n",
    "        \"\"\" 添加n个进度\n",
    "\n",
    "        :param n: 进度的个数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.is_run:\n",
    "            self.n_current += n\n",
    "            if self.n_current > self.n_print * self.n_split:\n",
    "                self.n_print += 1\n",
    "                if self.n_print > self.n_cols:\n",
    "                    self.n_print = self.n_cols\n",
    "            self._print()\n",
    "\n",
    "    def setDesc(self, desc):\n",
    "        \"\"\" 添加打印信息 \"\"\"\n",
    "        self.desc = desc\n",
    "\n",
    "    def _print(self):\n",
    "        des_info = \"\\r{0}: {1:>3d}% |\".format(self.desc, int(self.n_current / self.total * 100))\n",
    "        des_info += \"*\" * self.n_print + \"-\" * (self.n_cols - self.n_print)\n",
    "        des_info += \"| {0}/{1}\".format(self.n_current, self.total)\n",
    "        print(des_info, end=\"\")\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\" 结束进度条 \"\"\"\n",
    "        self.n_split = float(self.total) / float(self.n_split)\n",
    "        self.n_current = 0\n",
    "        self.n_print = 0\n",
    "        self.is_run = False\n",
    "        print()\n",
    "\n",
    "\n",
    "class RumTime:\n",
    "\n",
    "    def __init__(self, n_all=0):\n",
    "        self.n_all = n_all\n",
    "        self.n_current = 0\n",
    "        self.strat_time = time.time()\n",
    "        self.current_time = time.time()\n",
    "\n",
    "    def strat(self):\n",
    "        self.n_current = 0\n",
    "        self.strat_time = time.time()\n",
    "        self.current_time = time.time()\n",
    "\n",
    "    def add(self, n=1):\n",
    "        self.n_current += 1\n",
    "        self.current_time = time.time()\n",
    "\n",
    "    def printInfo(self):\n",
    "        out_s = f\"+ {self.n_current}\"\n",
    "        # time.strftime('%Y-%m-%d %H-%M-%S', time.localtime())\n",
    "        out_s += \" RUN:\"\n",
    "        out_s += RumTime.fmtTime(self.current_time - self.strat_time)\n",
    "        if self.n_all != 0:\n",
    "            out_s += \" ALL:\"\n",
    "            t1 = (self.current_time - self.strat_time) / (self.n_current + 0.0000001) * self.n_all\n",
    "            out_s += RumTime.fmtTime(t1)\n",
    "        print(out_s)\n",
    "\n",
    "    def end(self):\n",
    "        print(\"end\")\n",
    "\n",
    "    @classmethod\n",
    "    def fmtTime(cls, t):\n",
    "        hours = t // 3600\n",
    "        minutes = (t - 3600 * hours) // 60\n",
    "        seconds = t - 3600 * hours - minutes * 60\n",
    "        return f\"({int(hours)}:{int(minutes)}:{seconds:.2f})\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 33.          47.          41.          38.         159.          20.75471697]\n",
      " [ 42.          37.          30.          39.         148.          24.99999998]\n",
      " [ 34.          30.          40.          34.         138.          28.98550723]\n",
      " [ 30.          49.          45.          35.         159.          22.0125786 ]\n",
      " [139.         163.         156.         146.         604.          99.99999998]\n",
      " [ 23.74100718  22.69938649  25.64102562  23.97260272  99.99999998  24.00662252]]\n",
      "      CM       IS      VEG     SOIL    WATER      SUM       PA\n",
      "      IS       33       47       41       38      159    20.75\n",
      "     VEG       42       37       30       39      148    25.00\n",
      "    SOIL       34       30       40       34      138    28.99\n",
      "   WATER       30       49       45       35      159    22.01\n",
      "     SUM      139      163      156      146      604   100.00\n",
      "      UA    23.74    22.70    25.64    23.97   100.00    24.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(30, 50, [4, 4])\n",
    "names = [\"IS\", \"VEG\", \"SOIL\", \"WATER\"]\n",
    "cm = calCM(a)\n",
    "print(cm)\n",
    "print(fmtCM(cm, names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class SRTFeature:\n",
    "    \"\"\"\n",
    "    获得不同特征的名字\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_end = False\n",
    "        self.i_feat_types = []  # 特征类型组合的索引\n",
    "        with open(r\"full_extraction.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                lines = line.split(\" \")\n",
    "                self.i_feat_types.append(list(map(eval, lines)))\n",
    "                self.i_feat_types[-1] = list(map(lambda x: x - 1, self.i_feat_types[-1]))\n",
    "        self.i_current = 0\n",
    "        self.feat_types = []\n",
    "\n",
    "    def get(self):\n",
    "        if self.i_current < len(self.i_feat_types):\n",
    "            self.feat_types = list(feat_type_names[self.i_feat_types[self.i_current]])\n",
    "            self.i_current += 1\n",
    "            return self.get_feat_names()\n",
    "        else:\n",
    "            self.feat_types = []\n",
    "            return None\n",
    "\n",
    "    def clear(self):\n",
    "        self.i_current = 0\n",
    "\n",
    "    def get_feat_names(self, return_names=False):\n",
    "        feat_index = []\n",
    "        for feat_type in self.feat_types:\n",
    "            if feat_type == \"RGBN\":  # 1 可见光和近红外\n",
    "                feat_index.extend(['B', 'G', 'R', 'N'])\n",
    "            elif feat_type == \"SI\":  # 2 光谱指数\n",
    "                feat_index.extend(['NDVI', 'NDWI'])\n",
    "            elif feat_type == \"ASVVVH\":  # 3 升轨VV和VH\n",
    "                feat_index.extend(['VV_AS', 'VH_AS'])\n",
    "            elif feat_type == \"DEVVVH\":  # 4 降轨VV和VH\n",
    "                feat_index.extend(['VV_DE', 'VH_DE'])\n",
    "            elif feat_type == \"ASVVVHGLCM\":  # 5 升轨VV和VH纹理\n",
    "                feat_index.extend(['VH_AS_Mean', 'VH_AS_Variance', 'VH_AS_Homogeneity', 'VV_AS_Mean', 'VV_AS_Variance',\n",
    "                                   'VV_AS_Homogeneity'])\n",
    "            elif feat_type == \"DEVVVHGLCM\":  # 6 降轨VV和VH纹理\n",
    "                feat_index.extend(['VH_DE_Mean', 'VH_DE_Variance', 'VH_DE_Homogeneity', 'VV_DE_Mean', 'VV_DE_Variance',\n",
    "                                   'VV_DE_Homogeneity'])\n",
    "            elif feat_type == \"PCGLCM\":  # 7 降轨VV和VH纹理\n",
    "                feat_index.extend(['VH_DE_Mean', 'VH_DE_Variance', 'VH_DE_Homogeneity', 'VV_DE_Mean', 'VV_DE_Variance',\n",
    "                                   'VV_DE_Homogeneity'])\n",
    "            elif feat_type == \"CAS\":  # 8 升轨协方差矩阵\n",
    "                feat_index.extend(['AS_20210507_C22', 'AS_20210507_C12real', 'AS_20210507_C12imag', 'AS_20210507_C11'])\n",
    "            elif feat_type == \"CDE\":  # 9 升轨协方差矩阵\n",
    "                feat_index.extend(['DE_20210430_C22', 'DE_20210430_C12real', 'DE_20210430_C12imag', 'DE_20210430_C11'])\n",
    "            else:\n",
    "                raise Exception(\"Not find feature type as \" + feat_type)\n",
    "        if return_names:\n",
    "            return \"_\".join(feat_index), feat_index\n",
    "        else:\n",
    "            return feat_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class SRTSample:\n",
    "\n",
    "    def __init__(self, train_csv_fn, test_csv_fn=None):\n",
    "        \"\"\" 样本管理工具\n",
    "\n",
    "        :param train_csv_fn: 训练数据csv问价\n",
    "        :param test_csv_fn: 测试数据csv文件\n",
    "        \"\"\"\n",
    "        self.test_feat_names = None\n",
    "        self.train_feat_names = None\n",
    "        self.d_o_test = None\n",
    "        self.d_o_train = None\n",
    "        self.train_csv_fn = train_csv_fn\n",
    "        self.test_csv_fn = test_csv_fn\n",
    "        self.feat_names = []\n",
    "        self.train_cate_codes = None\n",
    "        self.test_cate_codes = None\n",
    "        self.d_o = None\n",
    "        self.d_train = None\n",
    "        self.d_test = None\n",
    "        self.train_test = None\n",
    "        if self.test_csv_fn is None:\n",
    "            self.initFromCsv_train(self.train_csv_fn)\n",
    "        else:\n",
    "            self.initFromCsv_traintest(self.train_csv_fn, self.test_csv_fn)\n",
    "\n",
    "    def initFromCsv_train(self, train_csv_file):\n",
    "        self.d_o = pd.read_csv(train_csv_file)\n",
    "        self.train_test = self.d_o[\"TrainOrTest\"].values\n",
    "        # self.train_test = (np.random.random(len(self.d_o)) < 0.7).astype(\"int\")\n",
    "        self.feat_names = list(self.d_o.keys())\n",
    "        self.train_cate_codes = self.d_o[\"CATEGORY\"].values[self.train_test == 1]\n",
    "        self.test_cate_codes = self.d_o[\"CATEGORY\"].values[self.train_test == 0]\n",
    "        self.feat_names = self.feat_names[5:]\n",
    "        self.d_train = np.zeros([len(self.d_o), len(self.feat_names)])\n",
    "        for i, k in enumerate(self.feat_names):\n",
    "            self.d_train[:, i] = np.clip(self.d_o[k].values, re_hist[i, 0], re_hist[i, 1])\n",
    "            self.d_train[:, i] = (self.d_train[:, i] - re_hist[i, 0]) / (re_hist[i, 1] - re_hist[i, 0])\n",
    "        self.d_test = pd.DataFrame(data=self.d_train[self.train_test == 0], columns=self.feat_names)\n",
    "        self.d_train = pd.DataFrame(data=self.d_train[self.train_test == 1], columns=self.feat_names)\n",
    "\n",
    "    def initFromCsv_traintest(self, train_csv_fn, test_csv_fn):\n",
    "        self.d_o_train = pd.read_csv(train_csv_fn)\n",
    "        self.train_cate_codes = self.d_o_train[\"CATEGORY\"].values\n",
    "        self.train_feat_names = list(self.d_o_train.keys())[5:]\n",
    "        self.d_train = np.zeros([len(self.d_o_train), len(self.train_feat_names)])\n",
    "        for i, k in enumerate(self.train_feat_names):\n",
    "            self.d_train[:, i] = np.clip(self.d_o_train[k].values, re_hist[i, 0], re_hist[i, 1])\n",
    "            self.d_train[:, i] = (self.d_train[:, i] - re_hist[i, 0]) / (re_hist[i, 1] - re_hist[i, 0])\n",
    "        self.d_train = pd.DataFrame(data=self.d_train, columns=self.train_feat_names)\n",
    "        self.d_o_test = pd.read_csv(test_csv_fn)\n",
    "        self.test_cate_codes = self.d_o_test[\"CATEGORY\"].values\n",
    "        self.test_feat_names = list(self.d_o_test.keys())[5:]\n",
    "        self.d_test = np.zeros([len(self.d_o_test), len(self.test_feat_names)])\n",
    "        for i, k in enumerate(self.test_feat_names):\n",
    "            self.d_test[:, i] = np.clip(self.d_o_test[k].values, re_hist[i, 0], re_hist[i, 1])\n",
    "            self.d_test[:, i] = (self.d_test[:, i] - re_hist[i, 0]) / (re_hist[i, 1] - re_hist[i, 0])\n",
    "        self.d_test = pd.DataFrame(data=self.d_test, columns=self.test_feat_names)\n",
    "        pass\n",
    "\n",
    "    def get(self, spl_type, feat_index):\n",
    "        \"\"\" 使用样本类型和特征类型作为样本获取标识\n",
    "\n",
    "        :param spl_type: 样本类型 SPLS, NOSPLS\n",
    "        :param feat_index: 特征类型\n",
    "        :return: 样本和标签\n",
    "        \"\"\"\n",
    "        d_index = np.array([True for i in range(len(self.d_train))])\n",
    "        if spl_type == \"NOSPLS\":\n",
    "            d_index = (self.train_cate_codes == 11) + (self.train_cate_codes == 21) \\\n",
    "                      + (self.train_cate_codes == 31) + (self.train_cate_codes == 41)\n",
    "        d_train: pd.DataFrame = self.d_train[feat_index][d_index]\n",
    "        labels_train: np.ndarray = self.train_cate_codes[d_index]\n",
    "        labels_train = np.floor(labels_train / 10)\n",
    "        d_test: pd.DataFrame = self.d_test[feat_index]\n",
    "        labels_test: np.ndarray = self.test_cate_codes\n",
    "        return d_train, labels_train, d_test, labels_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class SRTClassAlgos:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cm = None\n",
    "        self.test_acc = None\n",
    "        self.train_acc = None\n",
    "        self.model = None\n",
    "\n",
    "    def SVM(self, x, y, x_test, y_test, save_file=None):\n",
    "        \"\"\" SVM 分类器\n",
    "\n",
    "        :param x: 训练数据\n",
    "        :param y: 训练数据标签\n",
    "        :param x_test: 测试数据\n",
    "        :param y_test: 测试数据标签\n",
    "        :param save_file: 保存的文件\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        svm_args = {\"kernel\": \"rbf\", \"gamma\": \"auto\", \"C\": 1}\n",
    "        refer_args = {}\n",
    "        svm_args, refer_args = SRTClassAlgos.trainSvm(y, x)\n",
    "        algo_infos = {\n",
    "            \"algo_type\": \"RF\",\n",
    "            \"algo_args\": svm_args,\n",
    "            \"train_accuracy\": 0,\n",
    "            \"test_accuracy\": 0,\n",
    "            \"train_cm\": None,\n",
    "            \"test_cm\": None,\n",
    "            \"refer_args\": refer_args\n",
    "        }\n",
    "        self.model = SVC(\n",
    "            kernel=svm_args[\"kernel\"],\n",
    "            C=svm_args[\"C\"],\n",
    "            gamma=svm_args[\"gamma\"],\n",
    "            random_state=90)\n",
    "        self.model.fit(x, y)\n",
    "        self.train_acc = self.model.score(x, y)\n",
    "        self.test_acc = self.model.score(x_test, y_test)\n",
    "        algo_infos[\"train_accuracy\"] = self.train_acc\n",
    "        algo_infos[\"test_accuracy\"] = self.test_acc\n",
    "        self.cm = confusion_matrix(y_true=y_test, y_pred=self.model.predict(x_test))\n",
    "        self.cm = calCM(self.cm)\n",
    "        train_cm = confusion_matrix(y_true=y, y_pred=self.model.predict(x))\n",
    "        algo_infos[\"train_cm\"] = calCM(train_cm).tolist()\n",
    "        algo_infos[\"test_cm\"] = self.cm.tolist()\n",
    "        if save_file is not None:\n",
    "            joblib.dump(self.model, save_file)\n",
    "        return algo_infos\n",
    "\n",
    "    def RF(self, x, y, x_test, y_test, save_file=None):\n",
    "        \"\"\" Random Forest 分类器\n",
    "\n",
    "        :param x: 训练数据\n",
    "        :param y: 训练数据标签\n",
    "        :param x_test: 测试数据\n",
    "        :param y_test: 测试数据标签\n",
    "        :param save_file: 保存的文件\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rf_args = {\"n_estimators\": 100, \"max_depth\": 8, \"min_samples_leaf\": 1, \"min_samples_split\": 18}\n",
    "        refer_args = {}\n",
    "        rf_args, refer_args = SRTClassAlgos.trainRF(y, x)\n",
    "        algo_infos = {\n",
    "            \"algo_type\": \"RF\",\n",
    "            \"algo_args\": rf_args,\n",
    "            \"train_accuracy\": 0,\n",
    "            \"test_accuracy\": 0,\n",
    "            \"train_cm\": None,\n",
    "            \"test_cm\": None,\n",
    "            \"refer_args\": refer_args\n",
    "        }\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=rf_args[\"n_estimators\"],\n",
    "            max_depth=rf_args[\"max_depth\"],\n",
    "            min_samples_leaf=rf_args[\"min_samples_leaf\"],\n",
    "            min_samples_split=rf_args[\"min_samples_split\"],\n",
    "            random_state=90)\n",
    "        self.model.fit(x, y)\n",
    "        self.train_acc = self.model.score(x, y)\n",
    "        self.test_acc = self.model.score(x_test, y_test)\n",
    "        algo_infos[\"train_accuracy\"] = self.train_acc\n",
    "        algo_infos[\"test_accuracy\"] = self.test_acc\n",
    "        self.cm = confusion_matrix(y_true=y_test, y_pred=self.model.predict(x_test))\n",
    "        self.cm = calCM(self.cm)\n",
    "        train_cm = confusion_matrix(y_true=y, y_pred=self.model.predict(x))\n",
    "        algo_infos[\"train_cm\"] = calCM(train_cm).tolist()\n",
    "        algo_infos[\"test_cm\"] = self.cm.tolist()\n",
    "        if save_file is not None:\n",
    "            joblib.dump(self.model, save_file)\n",
    "        return algo_infos\n",
    "\n",
    "    @classmethod\n",
    "    def trainRF(cls, labels, d_train):\n",
    "        rf_args = {\"n_estimators\": 100, \"max_depth\": 8, \"min_samples_leaf\": 1, \"min_samples_split\": 18}\n",
    "        refer_args_infos = {}\n",
    "        # Tuning parameters: n_estimators -----------------------------------------\n",
    "        print(\"n_estimators: \", end=\"\")\n",
    "        scorel, s_max, canshu = [], 0, list(range(1, 150, 10))\n",
    "        for i in canshu:\n",
    "            rfc = RandomForestClassifier(n_estimators=i, n_jobs=-1, random_state=90)\n",
    "            score = cross_val_score(rfc, d_train, labels, cv=10).mean()\n",
    "            scorel.append(score)\n",
    "            print(f\"{i}:{score * 100:.2f}\", end=\" \")\n",
    "            if score > s_max:\n",
    "                s_max = score\n",
    "                rf_args[\"n_estimators\"] = i\n",
    "        refer_args_infos[\"n_estimators\"] = {\"accuracy\": scorel, \"args\": canshu}\n",
    "        print(\"\\n  -> \", max(scorel) * 100, rf_args[\"n_estimators\"])\n",
    "        # Tuning parameters: max_depth --------------------------------------------\n",
    "        print(\"max_depth: \", end=\"\")\n",
    "        scorel, s_max, canshu = [], 0, list(range(1, 20))\n",
    "        for i in canshu:\n",
    "            rfc = RandomForestClassifier(\n",
    "                n_estimators=rf_args[\"n_estimators\"]\n",
    "                , max_depth=i\n",
    "                , n_jobs=-1, random_state=90)\n",
    "            score = cross_val_score(rfc, d_train, labels, cv=10).mean()\n",
    "            print(f\"{i}:{score * 100:.2f}\", end=\" \")\n",
    "            scorel.append(score)\n",
    "            if score > s_max:\n",
    "                s_max = score\n",
    "                rf_args[\"max_depth\"] = i\n",
    "        print(\"\\n  -> \", max(scorel), rf_args[\"max_depth\"])\n",
    "        refer_args_infos[\"max_depth\"] = {\"accuracy\": scorel, \"args\": canshu}\n",
    "        # Tuning parameters: min_samples_leaf -------------------------------------\n",
    "        print(\"min_samples_leaf: \", end=\"\")\n",
    "        scorel, s_max, canshu = [], 0, list(range(1, 5))\n",
    "        for i in canshu:\n",
    "            rfc = RandomForestClassifier(\n",
    "                n_estimators=rf_args[\"n_estimators\"]\n",
    "                , max_depth=rf_args[\"max_depth\"]\n",
    "                , min_samples_leaf=i\n",
    "                , n_jobs=-1, random_state=90)\n",
    "            score = cross_val_score(rfc, d_train, labels, cv=10).mean()\n",
    "            print(f\"{i}:{score * 100:.2f}\", end=\" \")\n",
    "            scorel.append(score)\n",
    "            if score > s_max:\n",
    "                s_max = score\n",
    "                rf_args[\"min_samples_leaf\"] = i\n",
    "        print(\"\\n  -> \", max(scorel), rf_args[\"min_samples_leaf\"])\n",
    "        refer_args_infos[\"min_samples_leaf\"] = {\"accuracy\": scorel, \"args\": canshu}\n",
    "        # Tuning parameters: min_samples_split ------------------------------------\n",
    "        print(\"min_samples_split: \", end=\"\")\n",
    "        scorel, s_max, canshu = [], 0, list(range(2, 10))\n",
    "        for i in canshu:\n",
    "            rfc = RandomForestClassifier(\n",
    "                n_estimators=rf_args[\"n_estimators\"]\n",
    "                , max_depth=rf_args[\"max_depth\"]\n",
    "                , min_samples_leaf=rf_args[\"min_samples_leaf\"]\n",
    "                , min_samples_split=i\n",
    "                , n_jobs=-1, random_state=90)\n",
    "            score = cross_val_score(rfc, d_train, labels, cv=10).mean()\n",
    "            print(f\"{i}:{score * 100:.2f}\", end=\" \")\n",
    "            scorel.append(score)\n",
    "            if score > s_max:\n",
    "                s_max = score\n",
    "                rf_args[\"min_samples_split\"] = i\n",
    "        print(\"\\n  -> \", max(scorel), rf_args[\"min_samples_split\"])\n",
    "        refer_args_infos[\"min_samples_split\"] = {\"accuracy\": scorel, \"args\": canshu}\n",
    "        return rf_args, refer_args_infos\n",
    "\n",
    "    @classmethod\n",
    "    def trainSvm(cls, labels, d_train):\n",
    "        svm_args = {\"kernel\": \"rbf\", \"gamma\": \"auto\", \"C\": 1}\n",
    "        refer_args_infos = {}\n",
    "        # 调线软间隔C\n",
    "        print(\"C: \", end=\"\")\n",
    "        s_max, scores = 0, []\n",
    "        C_range = np.linspace(0.01, 10, 20)\n",
    "        for i in C_range:\n",
    "            clf = SVC(\n",
    "                kernel=svm_args[\"kernel\"],\n",
    "                C=i,\n",
    "                cache_size=5000)\n",
    "            score = cross_val_score(clf, d_train, labels, cv=10).mean()\n",
    "            scores.append(score)\n",
    "            print(f\"{i:.3f}:{scores[-1] * 100:.2f}\", end=\" \")\n",
    "            if scores[-1] > s_max:\n",
    "                s_max = scores[-1]\n",
    "                svm_args[\"C\"] = i\n",
    "        refer_args_infos[\"C\"] = {\"accuracy\": scores, \"args\": C_range.tolist()}\n",
    "        print(\"\\n  -> \", s_max, svm_args[\"C\"])\n",
    "        # plt.close()\n",
    "        # plt.plot(C_range, scores)\n",
    "        # plt.savefig(\"../Data/C.png\")\n",
    "        # 调 gamma\n",
    "        print(\"gamma: \", end=\"\")\n",
    "        s_max, scores = 0, []\n",
    "        gamma_range = np.logspace(-1, 1, 20)\n",
    "        for i in gamma_range:\n",
    "            clf = SVC(\n",
    "                kernel=svm_args[\"kernel\"],\n",
    "                C=svm_args[\"C\"],\n",
    "                gamma=i,\n",
    "                cache_size=5000)\n",
    "            score = cross_val_score(clf, d_train, labels, cv=10).mean()\n",
    "            scores.append(score)\n",
    "            print(f\"{i:.3f}:{scores[-1] * 100:.2f}\", end=\" \")\n",
    "            if scores[-1] > s_max:\n",
    "                s_max = scores[-1]\n",
    "                svm_args[\"gamma\"] = i\n",
    "        refer_args_infos[\"gamma\"] = {\"accuracy\": scores, \"args\": gamma_range.tolist()}\n",
    "        print(\"\\n  -> \", s_max, svm_args[\"C\"])\n",
    "        # plt.close()\n",
    "        # plt.plot(gamma_range, scores)\n",
    "        # plt.savefig(\"../Data/gamma.png\")\n",
    "        return svm_args, refer_args_infos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class SRTClassifyImage:\n",
    "    \"\"\"\n",
    "    分类图像\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, imd_file):\n",
    "        \"\"\" 图像分类\n",
    "\n",
    "        :param imd_file: 图像文件\n",
    "        \"\"\"\n",
    "        self.imd = EnviFileIO(imd_file).readToArray(interleave=\"b,r,c\")\n",
    "        print(self.imd.shape)\n",
    "        self.class_envi = EnviFileIO()\n",
    "        self.class_envi.im_infos = envi_class_file_info\n",
    "        for i in range(self.imd.shape[0]):\n",
    "            self.imd[i] = np.clip(self.imd[i], re_hist[i, 0], re_hist[i, 1])\n",
    "            self.imd[i] = (self.imd[i] - re_hist[i, 0]) / (re_hist[i, 1] - re_hist[i, 0])\n",
    "        self.d_select = []\n",
    "        self.n_jdt_all = self.imd.shape[1]\n",
    "        self.n_jdt_duan1 = int(self.imd.shape[1] / 5)\n",
    "        self.n_jdt_duan2 = int(self.imd.shape[1] / 50)\n",
    "        self.imdc = np.zeros((self.imd.shape[1], self.imd.shape[2]))\n",
    "\n",
    "    def classify(self, mod, feat_index, save_image_file):\n",
    "        \"\"\" 分类\n",
    "\n",
    "        :param mod: 模型，带有predict接口\n",
    "        :param feat_index: 特征的索引\n",
    "        :param save_image_file: 保存的影像文件\n",
    "        \"\"\"\n",
    "        self.d_select = [np.where(feat_names_all == c)[0][0] for c in feat_index]\n",
    "        print(\"> Classify:\", save_image_file)\n",
    "        self.imdc = self.imdc * 0\n",
    "        print(\"  \", end=\"\")\n",
    "        for i in range(self.imd.shape[1]):\n",
    "            if i % self.n_jdt_duan1 == 0:\n",
    "                print((i // self.n_jdt_duan1) * 20, end=\"\")\n",
    "            if i % self.n_jdt_duan2 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            x = self.imd[self.d_select, i, :]\n",
    "            self.imdc[i, :] = mod.predict(x.T)\n",
    "        print(\"100\")\n",
    "        self.class_envi.saveToFile(self.imdc.astype(\"int8\"), out_file=save_image_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8100\\1743118093.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m \u001B[0mTEMP_DIR\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrftime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'%Y-%m-%d %H-%M-%S'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlocaltime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainModels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "def trainModels():\n",
    "    save_info = {\n",
    "        \"spl_type\": [],\n",
    "        \"feature_type\": [],\n",
    "        \"class_algo_type\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"model_file_name\": [],\n",
    "        \"imc_npy_file\": [],\n",
    "        \"imc_geo_file\": [],\n",
    "        \"confusion_matrix\": [],\n",
    "        \"algo_info_file\": []\n",
    "    }\n",
    "    save_dir = os.path.join(r\"..\\Mods\", TEMP_DIR)\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "        print(save_dir)\n",
    "        os.mkdir(save_dir)\n",
    "    json_file_name = os.path.join(save_dir, \"info.json\")\n",
    "\n",
    "    srt_sample = SRTSample(r\"cc1_spl1_rspl33.csv\", r\"sh_qd_cc1_testspl2_jy_d.csv\")  # 样本\n",
    "    srt_class_algo = SRTClassAlgos()  # 算法\n",
    "    srt_feature = SRTFeature()  # 特征\n",
    "    srt_classify_image = SRTClassifyImage(\"../ImageDeal/qd_rgbn_si_asdeC_raw.dat\")\n",
    "\n",
    "    spl_types = [\"SPLS\", \"NOSPLS\"]\n",
    "    class_algo_types = [\"SVM\", \"RF\"]\n",
    "\n",
    "    run_time = RumTime(511 * len(spl_types) * len(class_algo_types))\n",
    "    run_time.strat()\n",
    "    ii = 0\n",
    "\n",
    "    for spl_type in spl_types:\n",
    "        for class_algo_type in class_algo_types:\n",
    "            srt_feature.clear()\n",
    "            while True:\n",
    "                feat_index = srt_feature.get()\n",
    "                # print(feat_index)\n",
    "                if feat_index is None:\n",
    "                    break\n",
    "                ii += 1\n",
    "                feat_type = \"_\".join(srt_feature.feat_types)\n",
    "                x0, y0, x_test0, y_test0 = srt_sample.get(spl_type, feat_index)\n",
    "                to_f = os.path.join(save_dir, \"_\".join([spl_type, class_algo_type, feat_type]))\n",
    "                save_info[\"spl_type\"].append(spl_type)\n",
    "                save_info[\"feature_type\"].append(feat_type)\n",
    "                save_info[\"class_algo_type\"].append(class_algo_type)\n",
    "                save_info[\"model_file_name\"].append(to_f + \"_mod.model\")\n",
    "                save_info[\"imc_npy_file\"].append(to_f + \"_d.npy\")\n",
    "                save_info[\"imc_geo_file\"].append(to_f + \"_imc.dat\")\n",
    "                save_info[\"algo_info_file\"].append(to_f + \"_ainfo.json\")\n",
    "                print(ii, \"> \", \" | \".join([spl_type, class_algo_type, feat_type]))\n",
    "                is_save_mod = save_info[\"model_file_name\"][-1]\n",
    "                # 模型训练\n",
    "                algo_info = {}\n",
    "                if class_algo_type == \"RF\":\n",
    "                    algo_info = srt_class_algo.RF(x0.values, y0, x_test0.values, y_test0, is_save_mod)\n",
    "                elif class_algo_type == \"SVM\":\n",
    "                    algo_info = srt_class_algo.SVM(x0.values, y0, x_test0.values, y_test0, is_save_mod)\n",
    "                else:\n",
    "                    raise Exception(\"Can not find class type \" + class_algo_type)\n",
    "                # 影像分类\n",
    "                srt_classify_image.classify(srt_class_algo.model, feat_index, save_info[\"imc_geo_file\"][-1])\n",
    "                algo_info[\"spl_type\"] = spl_type\n",
    "                algo_info[\"feature_type\"] = feat_type\n",
    "                algo_info[\"class_algo_type\"] = class_algo_type\n",
    "                with open(save_info[\"algo_info_file\"][-1], \"w\", encoding=\"utf-8\") as fs:\n",
    "                    json.dump(algo_info, fs)\n",
    "                save_info[\"train_accuracy\"].append(srt_class_algo.train_acc * 100)\n",
    "                save_info[\"test_accuracy\"].append(srt_class_algo.test_acc * 100)\n",
    "                save_cm_file = os.path.join(save_dir, \"cm.txt\")\n",
    "                n = saveCM(srt_class_algo.cm, save_cm_file, cate_names=[\"IS\", \"VEG\", \"SOIL\", \"WATER\"],\n",
    "                           infos=[spl_type, class_algo_type, feat_type])\n",
    "                save_info[\"confusion_matrix\"].append(n)\n",
    "                print(\"* test_accuracy:{:>6.3f}\".format(save_info[\"test_accuracy\"][-1]))\n",
    "                print(\"* train_accuracy:{:>6.3f}\".format(save_info[\"train_accuracy\"][-1]))\n",
    "\n",
    "                run_time.add()\n",
    "                run_time.printInfo()\n",
    "\n",
    "                print()\n",
    "\n",
    "\n",
    "    with open(json_file_name, 'w') as f:\n",
    "        json.dump(save_info, f)\n",
    "    df = pd.DataFrame(save_info)\n",
    "    df = df.sort_values(\"test_accuracy\", ascending=False)\n",
    "    print(df)\n",
    "    df.to_csv(os.path.join(save_dir, \"info.csv\"), index=False)\n",
    "\n",
    "    return df\n",
    " \n",
    "TEMP_DIR = time.strftime('%Y-%m-%d %H-%M-%S', time.localtime())\n",
    "df = trainModels()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
